{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting from Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elasticsearch client allows us to dig deeper into the index we've generated from the Huggingface dataset, and to leverage more elastic native features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass  \n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Prompt the user to enter their Elastic Cloud ID and API Key securely\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic API Key: \")\n",
    "\n",
    "# Create an Elasticsearch client using the provided credentials\n",
    "client = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,  # cloud id can be found under deployment management\n",
    "    api_key=ELASTIC_API_KEY, # your username and password for connecting to elastic, found under Deplouments - Security\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index we built earlier is now on our Elastic cluster, so we can interact with it with the normal API calls.\n",
    "\n",
    "One benefit of indexing your data is that you no longer need to locally load the dataset, rather sending the search queries to run where your ES client is hosted insted of processing your computations locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who’s here from Twitter? #x #twitter\n",
      "you can take the twitter user out of twitter but you can't take twitter out of the twitter user\n",
      "Deixem os tópicos do Twitter no Twitter\n",
      ">opens twitter\n",
      ">holocaust_denial.mp3\n",
      ">closes twitter\n",
      ">deletes app\n",
      "Twitter failed to scare legacy verified accounts into paying for Twitter Blue https://mashable.com/article/twitter-legacy-verified-account-twitter-blue-subscribers\n"
     ]
    }
   ],
   "source": [
    "index_name=\"bluesky\"\n",
    "query={\n",
    "        \"match\": {\n",
    "            \"text\": \"twitter\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "response = client.search(index=index_name, query=query)\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"][0:5]:\n",
    "    print(hit['_source']['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the model side, this can also be done on the Elastic side rather than through the HuggingFace hub. \n",
    "\n",
    "Similarily to the index - we can leverage the storing & compute of your Elastic instance for your chosen LLMs then simply call upon them using either the elastic client or compatible huggingface functions.\n",
    "\n",
    "For example, we can start the Elasticsearch models registered on HuggingFace and deploy them in our Elasitc cluster rather than running them locally as in the previous phase. \n",
    "We will once again set up[the sentiment text classifier](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from eland/eland\n",
      "Digest: sha256:64adfcc89f42a63641dab2d9b459fd8a6196e73b51ad69d51d9eef8f3fe612da\n",
      "Status: Image is up to date for docker.elastic.co/eland/eland:latest\n",
      "docker.elastic.co/eland/eland:latest\n",
      "\u001b[1m\n",
      "What's Next?\n",
      "\u001b[0m  View a summary of image vulnerabilities and recommendations → \u001b[36mdocker scout quickview docker.elastic.co/eland/eland\u001b[0m\n",
      "2024-12-04 12:30:02,457 INFO : Establishing connection to Elasticsearch\n",
      "2024-12-04 12:30:02,516 INFO : Connected to cluster named 'fdcc4e10e5a34385884a3eda9350099a' (version: 8.15.2)\n",
      "2024-12-04 12:30:02,517 INFO : Loading HuggingFace transformer tokenizer and model 'cardiffnlp/twitter-roberta-base-sentiment'\n",
      "Downloading config.json: 100%|█████████████████| 747/747 [00:00<00:00, 57.5kB/s]\n",
      "Downloading vocab.json: 100%|████████████████| 899k/899k [00:00<00:00, 3.34MB/s]\n",
      "Downloading merges.txt: 100%|████████████████| 456k/456k [00:00<00:00, 2.40MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 150/150 [00:00<00:00, 166kB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████| 499M/499M [00:45<00:00, 11.0MB/s]\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "2024-12-04 12:31:08,427 INFO : Creating model with id 'cardiffnlp__twitter-roberta-base-sentiment'\n",
      "2024-12-04 12:31:09,498 INFO : Uploading model definition\n",
      "100%|█████████████████████████████████████| 476/476 [04:06<00:00,  1.93 parts/s]\n",
      "2024-12-04 12:35:16,061 INFO : Uploading model vocabulary\n",
      "2024-12-04 12:35:16,792 INFO : Starting model deployment\n",
      "2024-12-04 12:35:26,297 INFO : Model successfully imported with id 'cardiffnlp__twitter-roberta-base-sentiment'\n"
     ]
    }
   ],
   "source": [
    "!docker pull docker.elastic.co/eland/eland\n",
    "\n",
    "!docker run -it --rm elastic/eland \\\n",
    "    eland_import_hub_model \\\n",
    "      --cloud-id $ELASTIC_CLOUD_ID \\\n",
    "      --es-api-key $ELASTIC_API_KEY \\\n",
    "      --hub-model-id cardiffnlp/twitter-roberta-base-sentiment \\\n",
    "      --task-type text_classification \\\n",
    "      --clear-previous \\\n",
    "      --start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will now show up under `Trainded Models` in your Elastic Cloud interface.\n",
    "\n",
    "We can now call this model and run inference tasks on it. Here's an example to get us started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 1,\n",
       " 'trained_model_configs': [{'model_id': 'cardiffnlp__twitter-roberta-base-sentiment',\n",
       "   'model_type': 'pytorch',\n",
       "   'created_by': 'api_user',\n",
       "   'version': '12.0.0',\n",
       "   'create_time': 1733315468654,\n",
       "   'model_size_bytes': 0,\n",
       "   'estimated_operations': 0,\n",
       "   'license_level': 'platinum',\n",
       "   'description': \"Model cardiffnlp/twitter-roberta-base-sentiment for task type 'text_classification'\",\n",
       "   'tags': [],\n",
       "   'input': {'field_names': ['text_field']},\n",
       "   'inference_config': {'text_classification': {'vocabulary': {'index': '.ml-inference-native-000002'},\n",
       "     'tokenization': {'roberta': {'do_lower_case': False,\n",
       "       'with_special_tokens': True,\n",
       "       'max_sequence_length': 512,\n",
       "       'truncate': 'first',\n",
       "       'span': -1,\n",
       "       'add_prefix_space': False}},\n",
       "     'classification_labels': ['LABEL_0', 'LABEL_1', 'LABEL_2'],\n",
       "     'num_top_classes': 0}},\n",
       "   'location': {'index': {'name': '.ml-inference-native-000002'}}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"cardiffnlp__twitter-roberta-base-sentiment\"\n",
    "models = client.ml.get_trained_models(model_id=model_id)\n",
    "models.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(result):\n",
    "    label = result[0][\"label\"]\n",
    "    if label == \"LABEL_1\":\n",
    "        label = \"neutral\"\n",
    "    elif label == \"LABEL_2\":\n",
    "        label = \"positive\"\n",
    "    elif label == \"LABEL_0\":\n",
    "        label = \"negative\"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = {'text_field': 'I love you'}\n",
    "\n",
    "result = client.ml.infer_trained_model(model_id =model_id, docs = doc_test)\n",
    "print(process_label(result[\"inference_results\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this principle and use an Elastic pipeline to run the inference on each of our saved blueskye posts in the original index; and adding the generated embeddings as a new field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ingest.put_pipeline(\n",
    "    id=\"pipeline_sentiment\",\n",
    "    processors=[\n",
    "        {\n",
    "            \"inference\": {\n",
    "                \"model_id\": model_id,\n",
    "                \"field_map\": {\"text\": \"text_field\"}, \n",
    "                \"target_field\": \"sentiment\",  \n",
    "            }\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"properties\" : {\n",
    "        \"text\" : {\n",
    "            \"type\" : \"keyword\",\n",
    "            \"type\" : \"text\"\n",
    "        },\n",
    "         #sentiment results field\n",
    "        \"sentiment\": {\n",
    "          \"properties\": {\n",
    "            \"model_id\": {\n",
    "              \"type\": \"text\",\n",
    "              \"fields\": {\n",
    "                \"keyword\": {\n",
    "                  \"type\": \"keyword\",\n",
    "                  \"ignore_above\": 256\n",
    "                  }\n",
    "              }\n",
    "            },\n",
    "            \"predicted_value\": {\n",
    "              \"type\": \"text\",\n",
    "              \"fields\": {\n",
    "                \"keyword\": {\n",
    "                  \"type\": \"keyword\",\n",
    "                  \"ignore_above\": 256\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"prediction_probability\": {\n",
    "              \"type\": \"float\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'bluesky_sentiment'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the index (deleting any existing index)\n",
    "client.indices.delete(index=\"bluesky_sentiment\", ignore_unavailable=True)\n",
    "client.indices.create(index=\"bluesky_sentiment\", mappings=mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'task': '3ym8dc4vSe68mqqnBZfPgg:132218057'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the new index with enriched data\n",
    "client.reindex(body={\n",
    "      \"source\": {\n",
    "          \"index\": \"bluesky\"},\n",
    "      \"dest\": {\"index\": \"bluesky_sentiment\", \"pipeline\" : \"pipeline_sentiment\"}\n",
    "    }, wait_for_completion=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will run in the background as it goes through all the bluesky posts to analyze their sentinent. You can check on the progress via the elastic dev console using:\n",
    "\n",
    "```\n",
    "GET _tasks/3ym8dc4vSe68mqqnBZfPgg:132218057\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this has finished, our entire index will now have the sentiment field.\n",
    "\n",
    "We can start building elasticsearch queries to sort through our data based on filters, matches, and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some or of most positive posts so far:\n",
      "What a warm welcome on Bluesky and what a joy & honor it's coming from you 🤩 Thank you so much!\n",
      "I’m loving this new platform.. Still trying to find all my favourite ladies on here.. Looking forward to having lots of fun..\n",
      "@tbonesprimecuts.bsky.social HAPPY BIRTHDAY!!! 🎂🎈🎉🎇🎆 I hope you have an absolutely fantastic day and a year full of happiness, joy, and good health. ♥️\n",
      "This looks pretty good so far, I can't wait to play it!\n",
      "LUNA IS SO PRECIOUS! 🥹\n",
      "\n",
      "happy 10th birthday to her! 🎉🙌🏻\n"
     ]
    }
   ],
   "source": [
    "query={\n",
    "    \"bool\":\n",
    "    {\n",
    "      \"must\": [\n",
    "      \n",
    "      {\n",
    "        \"match\" : {\n",
    "            \"sentiment.predicted_value\" : \"LABEL_2\"\n",
    "          }\n",
    "      }]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index = \"bluesky_sentiment\",query=query, size=5, sort=\"sentiment.prediction_probability:desc\")\n",
    "\n",
    "print(\"Some or of most positive posts so far:\")\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit['_source'][\"text\"] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
