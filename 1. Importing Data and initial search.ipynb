{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Elasticsearch to explore Huggingface Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting from a HuggingFace dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface allows as to quickly get started with datasets. This collection of 2 million posts from blueskye will allow us to explore the text social media data and find some cool insights. \n",
    "\n",
    "https://huggingface.co/datasets/alpindale/two-million-bluesky-posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iulia/Documents/search options/search_options/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"alpindale/two-million-bluesky-posts\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"This is really interesting polling data about national public attitudes re: California.  It's from the LA Times, in January.  I wonder if this will change substantially in the next two years?  5233025.fs1.hubspotusercontent-na1.net/hubfs/523302...\"],\n",
       " 'created_at': ['2024-11-27T07:53:47.202Z'],\n",
       " 'author': ['did:plc:5ug6fzthlj6yyvftj3alekpj'],\n",
       " 'uri': ['at://did:plc:5ug6fzthlj6yyvftj3alekpj/app.bsky.feed.post/3lbw33zxvik24'],\n",
       " 'has_images': [False],\n",
       " 'reply_to': [None]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting thing we can do with such a dataset is to search through the posts. Huggingface integrates seamlessly with elasticsearch to allow us to add search capabilities to the data. \n",
    "\n",
    "[These docs](https://huggingface.co/docs/datasets/en/faiss_es#elasticsearch) show how to add a search index to your Dataset.\n",
    "\n",
    "We can first connect to our cloud hosted ES client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass  \n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Prompt the user to enter their Elastic Cloud ID and API Key securely\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic API Key: \")\n",
    "\n",
    "# Create an Elasticsearch client using the provided credentials\n",
    "client = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,  # cloud id can be found under deployment management\n",
    "    api_key=ELASTIC_API_KEY, # your username and password for connecting to elastic, found under Deplouments - Security\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we now build an index out of our dataset to leverage for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2107530/2107530 [08:13<00:00, 4270.93docs/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'created_at', 'author', 'uri', 'has_images', 'reply_to'],\n",
       "    num_rows: 2107530\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name=\"bluesky\"\n",
    "ds.add_elasticsearch_index(column=\"text\", es_client=client ,es_index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This created the \"bluesky\" index in Elasticsearch and added our HuggingFace dataset to it. It also creates an index on the \"text\" feature of our Huggingface dataset that can be further leveraged.\n",
    "\n",
    "Once the index has been initialized once, you can load it again for future uses from elastic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=\"bluesky\"\n",
    "ds.load_elasticsearch_index(\"text\", es_client=client ,es_index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now quickly run some searches!\n",
    "It feels like one of the most talked about topics on bluesky (at least from what my feed looks like lately) is people discussing twitter. Let's test that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Armenia is an amazing destination, with incredible history and beautiful landscapes. As the only current nation on the world's oldest map, oldest winery site and the first #Christian nation, it's worth travelling to this inexpensive destination to discover places like this (Dadal's Bridge, 14th c)!\n",
      "\n",
      "Bleurgh!!!! Why is it that travelling anywhere involves so much, well, travelling?\n",
      "\n",
      "Destination reached ðŸ«¡\n",
      "\n",
      "Destination:  Afternoon Nap.\n",
      "\n",
      "Journey before destination.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores, retrieved_examples = ds.get_nearest_examples(index_name=\"text\", query=\"travelling destination\", k=5)\n",
    "for response in retrieved_examples[\"text\"]:\n",
    "    print(response + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if people on bluesky are talking about HuggingFace, or maybe our elasticon conference that happened last week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "following the success of the community track at #ElasticON (here amsterdam from yesterday): we still have paris, london, singapore, and sydney coming up and are looking for community talks. send us your best tech topics around anything #elastic\n",
      "https://sessionize.com/elasticon\n",
      "\n",
      "Attended ElasticOn yesterday in Amsterdam. And learned about Better Binary Quantization (BBQ), which recently got implemented in Elastic and Lucene. Based of RaBitQ paper. Very interesting for everybody busy with vector search and RAG systems. www.elastic.co/search-labs/...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores, retrieved_examples = ds.get_nearest_examples(index_name=\"text\", query=\"elasticon\", k=2)\n",
    "for response in retrieved_examples[\"text\"]:\n",
    "    print(response + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from these examples we can already think of what next steps would be interesting to explore. \n",
    "\n",
    "Perhaps checking the sentiment along with each query to see how people feel about a topic? \n",
    "\n",
    "Or using a multilingual model to help us work through the international posts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go beyond simple search by also leveraging models from the HuggingFace model hub to generate more insights for our queries.\n",
    "\n",
    "Let's start out with sentiment! For this task we can use [this classification model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment) trained on tweets which we can expect to work quite well since our data type will be quite similar.\n",
    "\n",
    "These are the labels it should generate:\n",
    "\n",
    "Labels: 0 -> Negative; 1 -> Neutral; 2 -> Positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test on some obvious data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty legitimate. Let's see how people feel about some topics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_2', 'score': 0.955704927444458},\n",
       " {'label': 'LABEL_0', 'score': 0.9654269218444824}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe([\"I love you\", \"I hate you\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(result):\n",
    "    label = result[0][\"label\"]\n",
    "    if label == \"LABEL_1\":\n",
    "        label = \"neutral\"\n",
    "    elif label == \"LABEL_2\":\n",
    "        label = \"positive\"\n",
    "    elif label == \"LABEL_0\":\n",
    "        label = \"negative\"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Armenia is an amazing destination, with incredible history and beautiful landscapes. As the only current nation on the world's oldest map, oldest winery site and the first #Christian nation, it's worth travelling to this inexpensive destination to discover places like this (Dadal's Bridge, 14th c)!\n",
      "Sentiment: positive\n",
      "\n",
      "Bleurgh!!!! Why is it that travelling anywhere involves so much, well, travelling?\n",
      "Sentiment: negative\n",
      "\n",
      "Destination reached ðŸ«¡\n",
      "Sentiment: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores, retrieved_examples = ds.get_nearest_examples(index_name=\"text\", query=\"travelling destination\", k=3)\n",
    "for result in retrieved_examples[\"text\"]:\n",
    "    print(result)\n",
    "    print(\"Sentiment: \" + process_label(pipe(result)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a great start, we can already see the model does okay with the sentiment for our searches. But to get some more meaningful insights, we'd need to scale this example out. \n",
    "\n",
    "At the moment our data and model are both stored / running locally and we'd need to re-run this notebook whenever we want to run a differet query. \n",
    "\n",
    "In the next part of the series we'll take a look at storing our data and model to be able to scale out the inference process and run more complex queries.\n",
    "\n",
    "\n",
    "Then we can start building some interesting insights like:\n",
    "* How popular is a particular topic? \n",
    "* Can we count posts or interest over time?\n",
    "* How about the general sentiment over the topic?\n",
    "* Could we also measure this in real time?\n",
    "\n",
    "\n",
    "Our current dataset will be a bit difficult to use to answer these questions in the current state.\n",
    "\n",
    "Here's where the Elastic index and inference service will come in handy to help us build a more complex search use case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
