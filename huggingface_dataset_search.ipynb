{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Elasticsearch to explore Huggingface Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the ES client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass  \n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Prompt the user to enter their Elastic Cloud ID and API Key securely\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic API Key: \")\n",
    "\n",
    "# Create an Elasticsearch client using the provided credentials\n",
    "client = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,  # cloud id can be found under deployment management\n",
    "    api_key=ELASTIC_API_KEY, # your username and password for connecting to elastic, found under Deplouments - Security\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface allows as to quickly get started with datasets. This collection of 2 million posts from blueskye will allow us to explore the text social media data and find some cool insights. \n",
    "\n",
    "https://huggingface.co/datasets/alpindale/two-million-bluesky-posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iulia/Documents/search options/search_options/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"alpindale/two-million-bluesky-posts\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"This is really interesting polling data about national public attitudes re: California.  It's from the LA Times, in January.  I wonder if this will change substantially in the next two years?  5233025.fs1.hubspotusercontent-na1.net/hubfs/523302...\"],\n",
       " 'created_at': ['2024-11-27T07:53:47.202Z'],\n",
       " 'author': ['did:plc:5ug6fzthlj6yyvftj3alekpj'],\n",
       " 'uri': ['at://did:plc:5ug6fzthlj6yyvftj3alekpj/app.bsky.feed.post/3lbw33zxvik24'],\n",
       " 'has_images': [False],\n",
       " 'reply_to': [None]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting thing we can do with such a dataset is to search through the posts. Huggingface integrates seamlessly with elasticsearch to allow us to add search capabilities to the data. \n",
    "\n",
    "[These docs](https://huggingface.co/docs/datasets/en/faiss_es#elasticsearch) show how to add a search index to your Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2107530/2107530 [08:13<00:00, 4270.93docs/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'created_at', 'author', 'uri', 'has_images', 'reply_to'],\n",
       "    num_rows: 2107530\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name=\"bluesky\"\n",
    "ds.add_elasticsearch_index(column=\"text\", es_client=client ,es_index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the index has been initialized once, you can load it again for future uses from elastic.\n",
    "ds.load_elasticsearch_index(\"text\", es_client=client ,es_index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This created the \"bluesky\" index in Elasticsearch and added our HuggingFace dataset to it. It also creates an index on the \"text\" feature of our Huggingface dataset that can be further leveraged.\n",
    "\n",
    "This means that we can run our usual commands to interact with this data through the regular elastic client (or any other methods like direct API calls or the Dev Console):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleurgh!!!! Why is it that travelling anywhere involves so much, well, travelling?\n",
      "i am TRAVELLING not DRIVING\n",
      "Travelling squad gonna be hilarious\n",
      "Very nice! Are you stull travelling?\n",
      "Amsterdam, Netherlands ðŸ‡³ðŸ‡±\n",
      "\n",
      "#Amsterdam #Holland #travelling\n"
     ]
    }
   ],
   "source": [
    "query={\n",
    "        \"match\": {\n",
    "            \"text\": \"travelling\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "response = client.search(index=index_name, query=query)\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"][0:5]:\n",
    "    print(hit['_source']['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can continue to use the huggingface functions and leverage the ES index that has been added to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bleurgh!!!! Why is it that travelling anywhere involves so much, well, travelling?',\n",
       " 'i am TRAVELLING not DRIVING',\n",
       " 'Travelling squad gonna be hilarious',\n",
       " 'Very nice! Are you stull travelling?',\n",
       " 'Amsterdam, Netherlands ðŸ‡³ðŸ‡±\\n\\n#Amsterdam #Holland #travelling']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, retrieved_examples = ds.get_nearest_examples(index_name=\"text\", query=\"travelling\", k=5)\n",
    "retrieved_examples[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of indexing your data however, is that you no longer need to locally load the dataset, rather sending the search queries to run where your ES client is hosted insted of processing your computations locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While simple search is already useful, we can leverage even more Huggingface + Elasticsearch superpowers by adding models into the mix. \n",
    "\n",
    "Similarily to the index - we can leverage the storing & compute of your Elastic instance for your chosen LLMs then simply call upon them using either the elastic client or compatible huggingface functions.\n",
    "\n",
    "For example, we can start with one of the Elasticsearch models registered on HuggingFace: [the E5 multilingual transofrmer](https://huggingface.co/elastic/multilingual-e5-small-optimized) will be useful to help us search through all posts, even in foreign languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull docker.elastic.co/eland/eland\n",
    "\n",
    "!docker run -it --rm elastic/eland \\\n",
    "    eland_import_hub_model \\\n",
    "      --cloud-id $ELASTIC_CLOUD_ID \\\n",
    "      --es-api-key $ELASTIC_API_KEY \\\n",
    "      --hub-model-id elastic/multilingual-e5-small-optimized \\\n",
    "      --task-type text_embedding \\\n",
    "      --clear-previous \\\n",
    "      --start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will now show up under `Trainded Models` in your Elastic Cloud interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call this model and run inference tasks on it. Here's an example to get us started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 1,\n",
       " 'trained_model_configs': [{'model_id': 'elastic__multilingual-e5-small-optimized',\n",
       "   'model_type': 'pytorch',\n",
       "   'created_by': 'api_user',\n",
       "   'version': '12.0.0',\n",
       "   'create_time': 1733235941414,\n",
       "   'model_size_bytes': 0,\n",
       "   'estimated_operations': 0,\n",
       "   'license_level': 'platinum',\n",
       "   'description': \"Model elastic/multilingual-e5-small-optimized for task type 'text_embedding'\",\n",
       "   'tags': [],\n",
       "   'input': {'field_names': ['text_field']},\n",
       "   'inference_config': {'text_embedding': {'vocabulary': {'index': '.ml-inference-native-000002'},\n",
       "     'tokenization': {'xlm_roberta': {'do_lower_case': False,\n",
       "       'with_special_tokens': True,\n",
       "       'max_sequence_length': 512,\n",
       "       'truncate': 'first',\n",
       "       'span': -1}},\n",
       "     'embedding_size': 384}},\n",
       "   'location': {'index': {'name': '.ml-inference-native-000002'}}}]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"elastic__multilingual-e5-small-optimized\"\n",
    "models = client.ml.get_trained_models(model_id=model_id)\n",
    "models.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiError",
     "evalue": "ApiError(500, 'status_exception', \"Error in inference process: [forward() is missing value for argument 'token_type_ids'. Declaration: forward(__torch__.eland.ml.pytorch.transformers.___torch_mangle_889._SentenceTransformerWrapper self, Tensor input_ids, Tensor attention_mask, Tensor token_type_ids, Tensor position_ids) -> Tensor\\nException raised from checkAndNormalizeInputs at /usr/src/pytorch/aten/src/ATen/core/function_schema_inl.h:413 (most recent call first):\\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x68 (0x7f2be1d5b3c8 in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libc10.so)\\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xf3 (0x7f2be1d09671 in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libc10.so)\\nframe #2: <unknown function> + 0x1476f7d (0x7f2be3235f7d in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libtorch_cpu.so)\\nframe #3: torch::jit::Method::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&) const + 0x16c (0x7f2be64a9e1c in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libtorch_cpu.so)\\nframe #4: <unknown function> + 0x19cb3 (0x562367ca2cb3 in ./pytorch_inference)\\nframe #5: <unknown function> + 0x1ac33 (0x562367ca3c33 in ./pytorch_inference)\\nframe #6: <unknown function> + 0x40194 (0x562367cc9194 in ./pytorch_inference)\\nframe #7: <unknown function> + 0x17c61 (0x562367ca0c61 in ./pytorch_inference)\\nframe #8: <unknown function> + 0x17d38 (0x562367ca0d38 in ./pytorch_inference)\\nframe #9: ml::core::CStaticThreadPool::CWrappedTask::operator()() + 0x2e (0x7f2be1c2288e in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libMlCore.so)\\nframe #10: ml::core::CStaticThreadPool::worker(unsigned long) + 0xac (0x7f2be1c22f3c in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libMlCore.so)\\nframe #11: <unknown function> + 0x109acf (0x7f2be1536acf in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libstdc++.so.6)\\nframe #12: <unknown function> + 0x8609 (0x7f2be18b9609 in /lib/x86_64-linux-gnu/libpthread.so.0)\\nframe #13: clone + 0x43 (0x7f2be1141353 in /lib/x86_64-linux-gnu/libc.so.6)\\n]\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Run a query againt the model - this is the format the query imput must be used in, you can later map your features into this format through an ingest pipeline\u001b[39;00m\n\u001b[1;32m      2\u001b[0m doc_test \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtext_field\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mi am TRAVELLING not DRIVING\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m----> 4\u001b[0m result \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mml\u001b[39m.\u001b[39;49minfer_trained_model(model_id \u001b[39m=\u001b[39;49mmodel_id, docs \u001b[39m=\u001b[39;49m doc_test)\n\u001b[1;32m      5\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39minference_results\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/search options/search_options/.venv/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:455\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[39mreturn\u001b[39;00m api(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/search options/search_options/.venv/lib/python3.10/site-packages/elasticsearch/_sync/client/ml.py:2680\u001b[0m, in \u001b[0;36mMlClient.infer_trained_model\u001b[0;34m(self, model_id, docs, error_trace, filter_path, human, inference_config, pretty, timeout, body)\u001b[0m\n\u001b[1;32m   2678\u001b[0m         __body[\u001b[39m\"\u001b[39m\u001b[39minference_config\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m inference_config\n\u001b[1;32m   2679\u001b[0m __headers \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39maccept\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent-type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m-> 2680\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_request(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   2681\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2682\u001b[0m     __path,\n\u001b[1;32m   2683\u001b[0m     params\u001b[39m=\u001b[39;49m__query,\n\u001b[1;32m   2684\u001b[0m     headers\u001b[39m=\u001b[39;49m__headers,\n\u001b[1;32m   2685\u001b[0m     body\u001b[39m=\u001b[39;49m__body,\n\u001b[1;32m   2686\u001b[0m     endpoint_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mml.infer_trained_model\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2687\u001b[0m     path_parts\u001b[39m=\u001b[39;49m__path_parts,\n\u001b[1;32m   2688\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/search options/search_options/.venv/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:423\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_request\u001b[39m(\n\u001b[1;32m    411\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    412\u001b[0m     method: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[39m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[39m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    424\u001b[0m         method,\n\u001b[1;32m    425\u001b[0m         path,\n\u001b[1;32m    426\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    427\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    428\u001b[0m         body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    429\u001b[0m         endpoint_id\u001b[39m=\u001b[39;49mendpoint_id,\n\u001b[1;32m    430\u001b[0m         path_parts\u001b[39m=\u001b[39;49mpath_parts,\n\u001b[1;32m    431\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/search options/search_options/.venv/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:271\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperform_request\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    257\u001b[0m     method: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     path_parts: Optional[Mapping[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    265\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ApiResponse[Any]:\n\u001b[1;32m    266\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_otel\u001b[39m.\u001b[39mspan(\n\u001b[1;32m    267\u001b[0m         method,\n\u001b[1;32m    268\u001b[0m         endpoint_id\u001b[39m=\u001b[39mendpoint_id,\n\u001b[1;32m    269\u001b[0m         path_parts\u001b[39m=\u001b[39mpath_parts \u001b[39mor\u001b[39;00m {},\n\u001b[1;32m    270\u001b[0m     ) \u001b[39mas\u001b[39;00m otel_span:\n\u001b[0;32m--> 271\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_perform_request(\n\u001b[1;32m    272\u001b[0m             method,\n\u001b[1;32m    273\u001b[0m             path,\n\u001b[1;32m    274\u001b[0m             params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    275\u001b[0m             headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    276\u001b[0m             body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    277\u001b[0m             otel_span\u001b[39m=\u001b[39;49motel_span,\n\u001b[1;32m    278\u001b[0m         )\n\u001b[1;32m    279\u001b[0m         otel_span\u001b[39m.\u001b[39mset_elastic_cloud_metadata(response\u001b[39m.\u001b[39mmeta\u001b[39m.\u001b[39mheaders)\n\u001b[1;32m    280\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/search options/search_options/.venv/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:352\u001b[0m, in \u001b[0;36mBaseClient._perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mKeyError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    350\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     \u001b[39mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[39m.\u001b[39mget(meta\u001b[39m.\u001b[39mstatus, ApiError)(\n\u001b[1;32m    353\u001b[0m         message\u001b[39m=\u001b[39mmessage, meta\u001b[39m=\u001b[39mmeta, body\u001b[39m=\u001b[39mresp_body\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[39m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verified_elasticsearch:\n\u001b[1;32m    358\u001b[0m     \u001b[39m# If the header is set we mark the server as verified.\u001b[39;00m\n",
      "\u001b[0;31mApiError\u001b[0m: ApiError(500, 'status_exception', \"Error in inference process: [forward() is missing value for argument 'token_type_ids'. Declaration: forward(__torch__.eland.ml.pytorch.transformers.___torch_mangle_889._SentenceTransformerWrapper self, Tensor input_ids, Tensor attention_mask, Tensor token_type_ids, Tensor position_ids) -> Tensor\\nException raised from checkAndNormalizeInputs at /usr/src/pytorch/aten/src/ATen/core/function_schema_inl.h:413 (most recent call first):\\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x68 (0x7f2be1d5b3c8 in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libc10.so)\\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xf3 (0x7f2be1d09671 in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libc10.so)\\nframe #2: <unknown function> + 0x1476f7d (0x7f2be3235f7d in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libtorch_cpu.so)\\nframe #3: torch::jit::Method::operator()(std::vector<c10::IValue, std::allocator<c10::IValue> >, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, c10::IValue, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, c10::IValue> > > const&) const + 0x16c (0x7f2be64a9e1c in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libtorch_cpu.so)\\nframe #4: <unknown function> + 0x19cb3 (0x562367ca2cb3 in ./pytorch_inference)\\nframe #5: <unknown function> + 0x1ac33 (0x562367ca3c33 in ./pytorch_inference)\\nframe #6: <unknown function> + 0x40194 (0x562367cc9194 in ./pytorch_inference)\\nframe #7: <unknown function> + 0x17c61 (0x562367ca0c61 in ./pytorch_inference)\\nframe #8: <unknown function> + 0x17d38 (0x562367ca0d38 in ./pytorch_inference)\\nframe #9: ml::core::CStaticThreadPool::CWrappedTask::operator()() + 0x2e (0x7f2be1c2288e in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libMlCore.so)\\nframe #10: ml::core::CStaticThreadPool::worker(unsigned long) + 0xac (0x7f2be1c22f3c in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libMlCore.so)\\nframe #11: <unknown function> + 0x109acf (0x7f2be1536acf in /usr/share/elasticsearch/modules/x-pack-ml/platform/linux-x86_64/bin/../lib/libstdc++.so.6)\\nframe #12: <unknown function> + 0x8609 (0x7f2be18b9609 in /lib/x86_64-linux-gnu/libpthread.so.0)\\nframe #13: clone + 0x43 (0x7f2be1141353 in /lib/x86_64-linux-gnu/libc.so.6)\\n]\")"
     ]
    }
   ],
   "source": [
    "#Run a query againt the model - this is the format the query imput must be used in, you can later map your features into this format through an ingest pipeline\n",
    "doc_test = {'text_field': 'i am TRAVELLING not DRIVING'}\n",
    "\n",
    "result = client.ml.infer_trained_model(model_id =model_id, docs = doc_test)\n",
    "result[\"inference_results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this principle and use an Elastic pipeline to run the inference on each of our saved blueskye posts in the original index; and adding the generated embeddings as a new field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ingest.put_pipeline(\n",
    "    id=\"pipeline_e5\",\n",
    "    processors=[\n",
    "        {\n",
    "            \"inference\": {\n",
    "                \"model_id\": model_id,\n",
    "                \"field_map\": {\"text\": \"text_field\"},  # field to embed: text\n",
    "                \"target_field\": \"text_embeddings\",  # embedded field: text_embeddings\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"properties\" : {\n",
    "        \"text\" : {\n",
    "            \"type\" : \"keyword\",\n",
    "            \"type\" : \"text\"\n",
    "        },\n",
    "        \"text_embeddings.predicted_value\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 768,\n",
    "            \"index\": \"true\",\n",
    "            \"similarity\": \"cosine\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index (deleting any existing index)\n",
    "client.indices.delete(index=\"bluesky_embedd\", ignore_unavailable=True)\n",
    "client.indices.create(index=\"bluesky_embedd\", mappings=mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.create(index=\"test_skye\")\n",
    "docs = [{'text': 'Bleurgh!!!! Why is it that travelling anywhere involves so much, well, travelling?'},\n",
    "{'text': 'i am TRAVELLING not DRIVING'},\n",
    "{'text': 'Travelling squad gonna be hilarious'},\n",
    "{'text': 'Very nice! Are you stull travelling?'},\n",
    "{'text': 'Amsterdam, Netherlands ðŸ‡³ðŸ‡±\\n\\n#Amsterdam #Holland #travelling'}]\n",
    "for doc in docs:\n",
    "    client.index(index=\"test_skye\", document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'task': 'VCXSZIvKQjSq81_QCeN32w:254538329'})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the new index with enriched data\n",
    "client.reindex(body={\n",
    "      \"source\": {\n",
    "          \"index\": \"test_skye\"},\n",
    "      \"dest\": {\"index\": \"test_skye_emb\", \"pipeline\" : \"pipeline_e5\"}\n",
    "    }, wait_for_completion=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively, you can run the model locally via the HuggingFace integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"elastic/multilingual-e5-small-optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.9414, 0.9525],\n",
      "        [0.9414, 1.0000, 0.9261],\n",
      "        [0.9525, 0.9261, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"This is random.\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
