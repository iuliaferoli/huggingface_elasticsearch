{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass  \n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# Prompt the user to enter their Elastic Cloud ID and API Key securely\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic API Key: \")\n",
    "\n",
    "# Create an Elasticsearch client using the provided credentials\n",
    "client = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,  # cloud id can be found under deployment management\n",
    "    api_key=ELASTIC_API_KEY, # your username and password for connecting to elastic, found under Deplouments - Security\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iulia/Documents/search options/search_options/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 23/23 [01:20<00:00,  3.51s/files]\n",
      "Generating train split: 100%|██████████| 2107530/2107530 [00:00<00:00, 2172877.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"alpindale/two-million-bluesky-posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"This is really interesting polling data about national public attitudes re: California.  It's from the LA Times, in January.  I wonder if this will change substantially in the next two years?  5233025.fs1.hubspotusercontent-na1.net/hubfs/523302...\"],\n",
       " 'created_at': ['2024-11-27T07:53:47.202Z'],\n",
       " 'author': ['did:plc:5ug6fzthlj6yyvftj3alekpj'],\n",
       " 'uri': ['at://did:plc:5ug6fzthlj6yyvftj3alekpj/app.bsky.feed.post/3lbw33zxvik24'],\n",
       " 'has_images': [False],\n",
       " 'reply_to': [None]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only extract the relevant fields, and we will process them into the relevant datatype to match our desired mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"properties\" : {\n",
    "        \"text\" : {\n",
    "            \"type\" : \"keyword\",\n",
    "            \"type\" : \"text\"\n",
    "        },\n",
    "        \"created_at\": {\n",
    "            \"type\": \"date\" \n",
    "        },\n",
    "        \"author\" : {\n",
    "            \"type\" : \"keyword\",\n",
    "            \"type\" : \"text\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_element(document, key):\n",
    "    if document[key]:\n",
    "        text=document[key][0],\n",
    "    else:\n",
    "        text=None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to convert DataFrame rows to Elasticsearch documents\n",
    "def df_to_doc(df, index_name):\n",
    "    for document in df:\n",
    "        yield dict(_index=index_name, \n",
    "                   text=first_element(document, \"text\")\n",
    "                   created_at=first_element(document,\"created_at\"),\n",
    "                   author=first_element(document, \"author\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m client\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mcreate(index\u001b[39m=\u001b[39mindex_name)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Use the Elasticsearch helpers.bulk() method to index the DataFrame data into Elasticsearch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m load \u001b[39m=\u001b[39m helpers\u001b[39m.\u001b[39;49mbulk(client, df_to_doc(ds[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], index_name))\n",
      "File \u001b[0;32m~/Documents/search options/search_options/.venv/lib/python3.10/site-packages/elasticsearch/helpers/actions.py:540\u001b[0m, in \u001b[0;36mbulk\u001b[0;34m(client, actions, stats_only, ignore_status, *args, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[39m# make streaming_bulk yield successful results so we can count them\u001b[39;00m\n\u001b[1;32m    539\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39myield_ok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m \u001b[39mfor\u001b[39;00m ok, item \u001b[39min\u001b[39;00m streaming_bulk(\n\u001b[1;32m    541\u001b[0m     client, actions, ignore_status\u001b[39m=\u001b[39mignore_status, span_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhelpers.bulk\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    542\u001b[0m ):\n\u001b[1;32m    543\u001b[0m     \u001b[39m# go through request-response pairs and detect failures\u001b[39;00m\n\u001b[1;32m    544\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ok:\n\u001b[1;32m    545\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stats_only:\n",
      "File \u001b[0;32m~/Documents/search options/search_options/.venv/lib/python3.10/site-packages/elasticsearch/helpers/actions.py:435\u001b[0m, in \u001b[0;36mstreaming_bulk\u001b[0;34m(client, actions, chunk_size, max_chunk_bytes, raise_on_error, expand_action_callback, raise_on_exception, max_retries, initial_backoff, max_backoff, yield_ok, ignore_status, retry_on_status, span_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m bulk_data: List[\n\u001b[1;32m    429\u001b[0m     Union[\n\u001b[1;32m    430\u001b[0m         Tuple[_TYPE_BULK_ACTION_HEADER],\n\u001b[1;32m    431\u001b[0m         Tuple[_TYPE_BULK_ACTION_HEADER, _TYPE_BULK_ACTION_BODY],\n\u001b[1;32m    432\u001b[0m     ]\n\u001b[1;32m    433\u001b[0m ]\n\u001b[1;32m    434\u001b[0m bulk_actions: List[\u001b[39mbytes\u001b[39m]\n\u001b[0;32m--> 435\u001b[0m \u001b[39mfor\u001b[39;00m bulk_data, bulk_actions \u001b[39min\u001b[39;00m _chunk_actions(\n\u001b[1;32m    436\u001b[0m     \u001b[39mmap\u001b[39m(expand_action_callback, actions),\n\u001b[1;32m    437\u001b[0m     chunk_size,\n\u001b[1;32m    438\u001b[0m     max_chunk_bytes,\n\u001b[1;32m    439\u001b[0m     serializer,\n\u001b[1;32m    440\u001b[0m ):\n\u001b[1;32m    441\u001b[0m     \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_retries \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    442\u001b[0m         to_retry: List[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/search options/search_options/.venv/lib/python3.10/site-packages/elasticsearch/helpers/actions.py:234\u001b[0m, in \u001b[0;36m_chunk_actions\u001b[0;34m(actions, chunk_size, max_chunk_bytes, serializer)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39mSplit actions into chunks by number or size, serialize them into strings in\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39mthe process.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m chunker \u001b[39m=\u001b[39m _ActionChunker(\n\u001b[1;32m    232\u001b[0m     chunk_size\u001b[39m=\u001b[39mchunk_size, max_chunk_bytes\u001b[39m=\u001b[39mmax_chunk_bytes, serializer\u001b[39m=\u001b[39mserializer\n\u001b[1;32m    233\u001b[0m )\n\u001b[0;32m--> 234\u001b[0m \u001b[39mfor\u001b[39;00m action, data \u001b[39min\u001b[39;00m actions:\n\u001b[1;32m    235\u001b[0m     ret \u001b[39m=\u001b[39m chunker\u001b[39m.\u001b[39mfeed(action, data)\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m ret:\n",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m, in \u001b[0;36mdf_to_doc\u001b[0;34m(df, index_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdf_to_doc\u001b[39m(df, index_name):\n\u001b[1;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m document \u001b[39min\u001b[39;00m df:\n\u001b[1;32m      4\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mdict\u001b[39m(_index\u001b[39m=\u001b[39mindex_name, \n\u001b[0;32m----> 5\u001b[0m                    text\u001b[39m=\u001b[39mdocument[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      6\u001b[0m                    created_at\u001b[39m=\u001b[39mdocument[\u001b[39m\"\u001b[39m\u001b[39mcreated_at\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m      7\u001b[0m                    author\u001b[39m=\u001b[39mdocument[\u001b[39m\"\u001b[39m\u001b[39mauthor\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "index_name = \"bluesky\"\n",
    "# Create the Elasticsearch index with the specified name\n",
    "client.indices.create(index=index_name, mappings=mappings)\n",
    "\n",
    "# Use the Elasticsearch helpers.bulk() method to index the DataFrame data into Elasticsearch\n",
    "load = helpers.bulk(client, df_to_doc(ds[\"train\"], index_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.inde"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
