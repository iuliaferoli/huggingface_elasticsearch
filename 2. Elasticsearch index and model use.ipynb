{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting from Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elasticsearch client allows us to dig deeper into the index we've generated from the Huggingface dataset, and to leverage more elastic native features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass  \n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Prompt the user to enter their Elastic Cloud ID and API Key securely\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic API Key: \")\n",
    "\n",
    "# Create an Elasticsearch client using the provided credentials\n",
    "client = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,  # cloud id can be found under deployment management\n",
    "    api_key=ELASTIC_API_KEY, # your username and password for connecting to elastic, found under Deplouments - Security\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index we built earlier is now on our Elastic cluster, so we can interact with it with the normal API calls.\n",
    "\n",
    "One benefit of indexing your data is that you no longer need to locally load the dataset, rather sending the search queries to run where your ES client is hosted insted of processing your computations locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Armenia is an amazing destination, with incredible history and beautiful landscapes. As the only current nation on the world's oldest map, oldest winery site and the first #Christian nation, it's worth travelling to this inexpensive destination to discover places like this (Dadal's Bridge, 14th c)!\n",
      "Bleurgh!!!! Why is it that travelling anywhere involves so much, well, travelling?\n",
      "Destination reached ðŸ«¡\n",
      "Destination:  Afternoon Nap.\n",
      "Journey before destination.\n"
     ]
    }
   ],
   "source": [
    "index_name=\"bluesky\"\n",
    "query={\n",
    "        \"match\": {\n",
    "            \"text\": \"travelling destination\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "response = client.search(index=index_name, query=query)\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"][0:5]:\n",
    "    print(hit['_source']['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the model side, this can also be done on the Elastic side rather than through the HuggingFace hub. \n",
    "\n",
    "Similarily to the index - we can leverage the storing & compute of your Elastic instance for your chosen LLMs then simply call upon them using either the elastic client or compatible huggingface functions.\n",
    "\n",
    "For example, we can start the Elasticsearch models registered on HuggingFace and deploy them in our Elasitc cluster rather than running them locally as in the previous phase. \n",
    "We will once again set up[the sentiment text classifier](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "latest: Pulling from eland/eland\n",
      "Digest: sha256:64adfcc89f42a63641dab2d9b459fd8a6196e73b51ad69d51d9eef8f3fe612da\n",
      "Status: Image is up to date for docker.elastic.co/eland/eland:latest\n",
      "docker.elastic.co/eland/eland:latest\n",
      "\u001b[1m\n",
      "What's Next?\n",
      "\u001b[0m  View a summary of image vulnerabilities and recommendations â†’ \u001b[36mdocker scout quickview docker.elastic.co/eland/eland\u001b[0m\n",
      "2024-12-04 12:30:02,457 INFO : Establishing connection to Elasticsearch\n",
      "2024-12-04 12:30:02,516 INFO : Connected to cluster named 'fdcc4e10e5a34385884a3eda9350099a' (version: 8.15.2)\n",
      "2024-12-04 12:30:02,517 INFO : Loading HuggingFace transformer tokenizer and model 'cardiffnlp/twitter-roberta-base-sentiment'\n",
      "Downloading config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 747/747 [00:00<00:00, 57.5kB/s]\n",
      "Downloading vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00<00:00, 3.34MB/s]\n",
      "Downloading merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 2.40MB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 166kB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499M/499M [00:45<00:00, 11.0MB/s]\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "2024-12-04 12:31:08,427 INFO : Creating model with id 'cardiffnlp__twitter-roberta-base-sentiment'\n",
      "2024-12-04 12:31:09,498 INFO : Uploading model definition\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 476/476 [04:06<00:00,  1.93 parts/s]\n",
      "2024-12-04 12:35:16,061 INFO : Uploading model vocabulary\n",
      "2024-12-04 12:35:16,792 INFO : Starting model deployment\n",
      "2024-12-04 12:35:26,297 INFO : Model successfully imported with id 'cardiffnlp__twitter-roberta-base-sentiment'\n"
     ]
    }
   ],
   "source": [
    "!docker pull docker.elastic.co/eland/eland\n",
    "\n",
    "!docker run -it --rm elastic/eland \\\n",
    "    eland_import_hub_model \\\n",
    "      --cloud-id $ELASTIC_CLOUD_ID \\\n",
    "      --es-api-key $ELASTIC_API_KEY \\\n",
    "      --hub-model-id cardiffnlp/twitter-roberta-base-sentiment \\\n",
    "      --task-type text_classification \\\n",
    "      --clear-previous \\\n",
    "      --start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will now show up under `Trainded Models` in your Elastic Cloud interface.\n",
    "\n",
    "We can now call this model and run inference tasks on it. Here's an example to get us started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 1,\n",
       " 'trained_model_configs': [{'model_id': 'cardiffnlp__twitter-roberta-base-sentiment',\n",
       "   'model_type': 'pytorch',\n",
       "   'created_by': 'api_user',\n",
       "   'version': '12.0.0',\n",
       "   'create_time': 1733315468654,\n",
       "   'model_size_bytes': 0,\n",
       "   'estimated_operations': 0,\n",
       "   'license_level': 'platinum',\n",
       "   'description': \"Model cardiffnlp/twitter-roberta-base-sentiment for task type 'text_classification'\",\n",
       "   'tags': [],\n",
       "   'input': {'field_names': ['text_field']},\n",
       "   'inference_config': {'text_classification': {'vocabulary': {'index': '.ml-inference-native-000002'},\n",
       "     'tokenization': {'roberta': {'do_lower_case': False,\n",
       "       'with_special_tokens': True,\n",
       "       'max_sequence_length': 512,\n",
       "       'truncate': 'first',\n",
       "       'span': -1,\n",
       "       'add_prefix_space': False}},\n",
       "     'classification_labels': ['LABEL_0', 'LABEL_1', 'LABEL_2'],\n",
       "     'num_top_classes': 0}},\n",
       "   'location': {'index': {'name': '.ml-inference-native-000002'}}}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"cardiffnlp__twitter-roberta-base-sentiment\"\n",
    "models = client.ml.get_trained_models(model_id=model_id)\n",
    "models.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(result):\n",
    "    label = result[0][\"predicted_value\"]\n",
    "    if label == \"LABEL_1\":\n",
    "        label = \"neutral\"\n",
    "    elif label == \"LABEL_2\":\n",
    "        label = \"positive\"\n",
    "    elif label == \"LABEL_0\":\n",
    "        label = \"negative\"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "doc_test = {'text_field': 'I love you'}\n",
    "\n",
    "result = client.ml.infer_trained_model(model_id =model_id, docs = doc_test)\n",
    "print(process_label(result[\"inference_results\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this principle and use an Elastic pipeline to run the inference on each of our saved blueskye posts in the original index; and adding the generated embeddings as a new field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.ingest.put_pipeline(\n",
    "    id=\"pipeline_sentiment\",\n",
    "    processors=[\n",
    "        {\n",
    "            \"inference\": {\n",
    "                \"model_id\": model_id,\n",
    "                \"field_map\": {\"text\": \"text_field\"}, \n",
    "                \"target_field\": \"sentiment\",  \n",
    "            }\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"properties\" : {\n",
    "        \"text\" : {\n",
    "            \"type\" : \"keyword\",\n",
    "            \"type\" : \"text\"\n",
    "        },\n",
    "         #sentiment results field\n",
    "        \"sentiment\": {\n",
    "          \"properties\": {\n",
    "            \"model_id\": {\n",
    "              \"type\": \"text\",\n",
    "              \"fields\": {\n",
    "                \"keyword\": {\n",
    "                  \"type\": \"keyword\",\n",
    "                  \"ignore_above\": 256\n",
    "                  }\n",
    "              }\n",
    "            },\n",
    "            \"predicted_value\": {\n",
    "              \"type\": \"text\",\n",
    "              \"fields\": {\n",
    "                \"keyword\": {\n",
    "                  \"type\": \"keyword\",\n",
    "                  \"ignore_above\": 256\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"prediction_probability\": {\n",
    "              \"type\": \"float\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'bluesky_sentiment'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the index (deleting any existing index)\n",
    "client.indices.delete(index=\"bluesky_sentiment\", ignore_unavailable=True)\n",
    "client.indices.create(index=\"bluesky_sentiment\", mappings=mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'task': '3ym8dc4vSe68mqqnBZfPgg:132218057'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the new index with enriched data\n",
    "client.reindex(body={\n",
    "      \"source\": {\n",
    "          \"index\": \"bluesky\"},\n",
    "      \"dest\": {\"index\": \"bluesky_sentiment\", \"pipeline\" : \"pipeline_sentiment\"}\n",
    "    }, wait_for_completion=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will run in the background as it goes through all the bluesky posts to analyze their sentinent. You can check on the progress via the elastic dev console using:\n",
    "\n",
    "```\n",
    "GET _tasks/3ym8dc4vSe68mqqnBZfPgg:132218057\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running some searches w/ sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this has finished, our entire index will now have the sentiment field.\n",
    "\n",
    "We can start building elasticsearch queries to sort through our data based on filters, matches, and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"match\" : {\n",
    "        \"sentiment.predicted_value\" : \"LABEL_2\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get back 76 results\n",
      "Some of the *negative* results on travelling destination so far:\n",
      "\n",
      "Bleurgh!!!! Why is it that travelling anywhere involves so much, well, travelling?\n",
      "\n",
      "i am TRAVELLING not DRIVING\n",
      "\n",
      "''Always the journey, never the destination.''\n",
      "#EliteDangerous\n",
      "\n",
      "So you believe everyone travelling through an airport should be strip searched?\n",
      "\n",
      "the pain of living in the middle of nowhere event-wise and hating travelling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"travelling destination\"\n",
    "\n",
    "query={\n",
    "    \"bool\":\n",
    "    {\n",
    "      \"must\": [\n",
    "      {\n",
    "        \"match\" : {\n",
    "            \"text\" : text\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "            \"match\" : {\n",
    "                \"sentiment.predicted_value\" : \"LABEL_0\"\n",
    "            }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index = \"bluesky_sentiment\",query=query, size=5)\n",
    "\n",
    "\n",
    "print(\"We get back {total} results\".format(total=response[\"hits\"]['total']['value']))\n",
    "\n",
    "\n",
    "print(\"Some of the *negative* results on \" + text +\" so far:\")\n",
    "print()\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit['_source'][\"text\"] )\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The *most positive* results on travelling destination so far:\n",
      "\n",
      "Had the most amazing adventure travelling in Thailand. So much to learn about and appreciate. And, wow, November is a great time for us Brits to visit ðŸ¤—ðŸ™\n",
      "\n",
      "10. I love going on holiday and visiting new places. Started cruising last year and I love it so much. You get to visit so many different places whilst on the one holiday and best of all if you find a place you love on a day off the ship you know where to book if you do fancy a single destination!\n",
      "\n",
      "Happy Thanksgiving(tomorrow l will be travelling) to my friends and relatives south of the border. May you know you are blessed. â¤ï¸â¤ï¸\n",
      "\n",
      "Woke up in what may be the 11th hotel in 6 weeks of travelling for work....and the in-room coffee maker makes a 4 CUP CARAFE!!\n",
      "\n",
      "Sorry...so excited to just reach over and pour a second cup while joy-scrolling on #bluesky\n",
      "\n",
      "Today is ALREADY a good day.\n",
      "\n",
      "Glad you made it to your destination safely! Have fun, eat good food and pass out on the couch!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query={\n",
    "    \"bool\":\n",
    "    {\n",
    "      \"must\": [\n",
    "      {\n",
    "        \"match\" : {\n",
    "            \"text\" : text\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "            \"match\" : {\n",
    "                \"sentiment.predicted_value\" : \"LABEL_2\"\n",
    "            }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "}\n",
    "response = client.search(index = \"bluesky_sentiment\",query=query, size=5,  sort=\"sentiment.prediction_probability:desc\")\n",
    "print(\"The *most positive* results on \" + text +\" so far:\")\n",
    "print()\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit['_source'][\"text\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TBD - using semantic_text and the inference endpoint instead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
